---
title: 'Week 3: Polling and Public Opinion'
author: ShuXin Ho
date: '2024-09-22'
slug: week-3
categories: []
tags: []
---

```{r, include = FALSE}
# Load libraries and read data (processed FiveThirtyEight polling average datasets).

library(car)
library(caret)
library(CVXR)
library(glmnet)
library(missForest)
library(tidyverse)
library(ggrepel)
library(mice)
library(knitr)
library(blogdown)

d_pollav_natl <- read_csv("national_polls_1968-2024.csv")
d_pollav_state <- read_csv("state_polls_1968-2024.csv")
d_vote <- read_csv("popvote_1948-2020.csv")
d_vote$party[d_vote$party == "democrat"] <- "DEM"
d_vote$party[d_vote$party == "republican"] <- "REP"
d_pvstate_wide <- read_csv("clean_wide_state_2pv_1948_2020.csv")

# Shape and merge polling and election data using November polls.
d_poll_nov_state <- d_vote %>% 
  left_join(d_pollav_state %>% 
              group_by(year, party) %>% 
              top_n(1, poll_date) %>% 
              select(-candidate), 
            by = c("year", "party")) %>% 
  rename(nov_poll = poll_support) %>% 
  filter(year <= 2020) %>% 
  drop_na()
```

# Election Blog

## Week 3: Polling and Public Opinion

Leading up to the 2020 presidential elections, then-President Donald Trump [famously claimed](https://x.com/realDonaldTrump/status/1315638896550576131) on X (formerly Twitter) that he was "winning BIG in all of the polls that matter," dismissing the rest as "fake." This raises an important question: can we trust polls to reflect the real state of public opinion?

### Polling, explained

Polls are designed to **reveal the public’s preferences ahead of an election**. They help politicians gauge voter sentiment and allow the public to monitor general trends, which can lend credibility to the election process by reducing the chances of surprise results.

The goal of a poll is to offer an **unbiased** estimate of the population's opinion by surveying a **large, representative sample**. In theory, while some individuals may be at different ends of the extreme when answering polls, these biases should cancel out, and the aggregate result will provide a median estimate that closely reflects the true public opinion.

To further refine this, platforms like [FiveThirtyEight](https://projects.fivethirtyeight.com/polls/president-general/2024/national/) aggregate multiple polls. Individual polls can sometimes be biased towards certain populations or parties, for instance, one might overestimate support for a candidate, while another underestimates it. Therefore, **aggregating polls helps average out these errors** and gives us a clearer picture of the overall voter landscape.

### What can we learn from the polling averages over 2020 election cycle?

Though the media often describes certain events as "**gamechangers**," such as a candidate giving a notable speech or performing poorly for the presidential debate, political scientists argue that many **dynamic campaign elements may not matter as much as the underlying fundamentals**.

Looking at the graph below of polling averages during the 2020 election cycle, we can see this in action. While there were **some expected shifts after key events** like the Democratic National Convention (DNC) and the Republican National Convention (RNC), other moments showed surprising results. For example, despite expectations that Trump's COVID-19 diagnosis might significantly alter support for the Republican party, there was only a modest and short-lived bump in the polling numbers. On the Democratic side, approval fluctuated after events such as Ruth Bader Ginsburg's passing and the presidential debates, but there wasn't a dramatic shift that persisted. These trends suggest that even though certain events are billed as "gamechangers," their actual impact on polling is often limited or unpredictable, leaving many **fluctuations unexplained by these moments alone**.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
d_pollav_natl %>% 
  filter(year == 2020) %>% 
  ggplot(aes(x = poll_date, y = poll_support, color = party)) +
  geom_rect(xmin = as.Date("2020-08-17"), xmax = as.Date("2020-08-20"), ymin = 47.5, ymax = 100, alpha = 0.1, color = NA, fill = "grey") + 
  annotate("text", x = as.Date("2020-08-07"), y = 51.5, label = "DNC", size = 3) +
  geom_rect(xmin = as.Date("2020-08-24"), xmax = as.Date("2020-08-27"), ymin = 0, ymax = 47.2, alpha = 0.1, color = NA, fill = "grey") +
  annotate("text", x = as.Date("2020-09-04"), y = 45, label = "RNC", size = 3) +
  geom_rect(xmin = as.Date("2020-10-02"), xmax = as.Date("2020-10-12"), ymin = 0, ymax = 42.7, alpha = 0.05, color = NA, fill = "grey") +
  
  geom_point(size = 1) + 
  geom_line() + 
  
  geom_segment(x = as.Date("2020-03-12"), xend = as.Date("2020-03-12"), y = 0, yend = 44.8, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-03-12"), y = 42.5, label = "COVID \n Market Crash", size = 3) +
  geom_segment(x = as.Date("2020-04-08"), xend = as.Date("2020-04-08"), y = 49, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-03-25"), y = 51.3, label = "Bernie Ends Run", size = 3) +
  geom_segment(x = as.Date("2020-04-16"), xend = as.Date("2020-04-16"), y = 0, yend = 44, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-04-16"), y = 44.7, label = "22 mil \n Unemployment", size = 3) +
  geom_segment(x = as.Date("2020-05-27"), xend = as.Date("2020-05-27"), y = 0, yend = 43, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-06-05"), y = 44, label = "100k COVID Dead, \n George Floyd", size = 3) +
  
  geom_segment(x = as.Date("2020-07-14"), xend = as.Date("2020-07-14"), y = 0, yend = 50.3, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-06-19"), y = 47.5, label = "Moderna Announces", size = 3) +
  
  geom_segment(x = as.Date("2020-09-29"), xend = as.Date("2020-09-29"), y = 50, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-9-12"), y = 49.5, label = "Pres. Debate", size = 3) +
  geom_segment(x = as.Date("2020-10-07"), xend = as.Date("2020-10-07"), y = 51.7, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-10-17"), y = 50.3, label = "VP Debate", size = 3) +
  geom_segment(x = as.Date("2020-10-22"), xend = as.Date("2020-10-22"), y = 52, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-10-30"), y = 51.5, label = "Pres. Debate", size = 3) +
  annotate("text", x = as.Date("2020-10-15"), y = 43.7, label = "Trump Has COVID", size = 3) +
  geom_segment(x = as.Date("2020-09-18"), xend = as.Date("2020-09-18"), y = 50, yend = 100, linetype = "dashed", alpha = 0.4, color = "grey") +
  annotate("text", x = as.Date("2020-09-03"), y = 51.5, label = "RBG Passes", size = 3) +
  
  scale_x_date(date_labels = "%b %Y", date_breaks = "1 month") +
  scale_color_manual(values = c("dodgerblue4", "firebrick1")) +
  labs(x = "Date",
       y = "Average Poll Approval", 
       title = "Polling Averages by Date (2020)",
       caption = "Source: FiveThirtyEight") + 
  theme_classic() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

### Predicting 2024 election outcome using national polling data

To predict the 2024 election outcome, I will use the **regularized regression methods**, namely Ridge, Lasso, and Elastic Net. The regression plots below demonstrates how the coefficients for each variable (polling data from various weeks leading up to the election) change as I adjust the regularization strength (lambda) in each method. Ridge shows that all variables are kept in the model, though their influence decreases as lambda increases. Lasso reveals that some coefficients are driven to zero as lambda increases, indicating that certain weeks' polling data may not be as relevant. Elastic Net Regression offers a middle ground, where some coefficients are reduced to zero, but others are merely shrunk.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
# Read election results data. 
d_vote <- read_csv("popvote_1948-2020.csv")
d_vote$party[d_vote$party == "democrat"] <- "DEM"
d_vote$party[d_vote$party == "republican"] <- "REP"

# Shape and merge polling and election data using November polls. 
d_poll_nov <- d_vote %>% 
  left_join(d_pollav_natl %>% 
              group_by(year, party) %>% 
              top_n(1, poll_date) %>% 
              select(-candidate), 
            by = c("year", "party")) %>% 
  rename(nov_poll = poll_support) %>% 
  filter(year <= 2020) %>% 
  drop_na()

# OLS: Democratic candidates pv2p on November polling average. 
ols.nov.1 <- lm(pv2p ~ nov_poll, 
                data = subset(d_poll_nov, party == "DEM"))

# OLS: Party-stacked pv2p on November polling average.
ols.nov.2 <- lm(pv2p ~ nov_poll, 
                data = d_poll_nov)

# Create dataset of polling average by week until the election. 
d_poll_weeks <- d_pollav_natl %>% 
  group_by(year, party, weeks_left) %>%
  summarize(mean_poll_week = mean(poll_support)) %>% 
  filter(weeks_left <= 30) %>% 
  pivot_wider(names_from = weeks_left, values_from = mean_poll_week) %>% 
  left_join(d_vote, by = c("year", "party"))
 
# Split into training and testing data based on inclusion or exclusion of 2024. 
d_poll_weeks_train <- d_poll_weeks %>% 
  filter(year <= 2020)
d_poll_weeks_test <- d_poll_weeks %>% 
  filter(year == 2024)

colnames(d_poll_weeks)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_train)[3:33] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_test)[3:33] <- paste0("poll_weeks_left_", 0:30)

# Comparison of OLS and regularized regression methods. 
ols.pollweeks <- lm(paste0("pv2p ~ ", paste0( "poll_weeks_left_", 0:30, collapse = " + ")), 
                    data = d_poll_weeks_train)

# Separate data into X and Y for training. 
x.train <- d_poll_weeks_train %>%
  ungroup() %>% 
  select(all_of(starts_with("poll_weeks_left_"))) %>% 
  as.matrix()
y.train <- d_poll_weeks_train$pv2p

# Ridge. 
ridge.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 0) # Set ridge using alpha = 0.

# Lasso.
lasso.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 1) # Set lasso using alpha = 1.

# Elastic net.
enet.pollsweeks <- glmnet(x = x.train, y = y.train, alpha = 0.5) # Set elastic net using alpha = 0.5.

# Set up a plotting area with 1 row and 3 columns
par(mfrow = c(1, 3))

# Plot Ridge Regression
plot(ridge.pollsweeks, xvar = "lambda", main = "Ridge Regression")

# Plot Lasso Regression
plot(lasso.pollsweeks, xvar = "lambda", main = "Lasso Regression")

# Plot Elastic Net Regression
plot(enet.pollsweeks, xvar = "lambda", main = "Elastic Net Regression")

# Reset plotting layout to default
par(mfrow = c(1, 1))
```

The graph below compares the estimated coefficients across different methods. This gives an overview of which predictors (polling weeks) are most consistently influential in predicting the election outcome across various methods. Based on my analysis, I have predicted:

```{r, echo = FALSE, warning = FALSE, message = FALSE}
# Use cross-validated versions to find the optimal values of lambda that minimize the MSE of your predictions. 
cv.ridge.pollweeks <- cv.glmnet(x = x.train, y = y.train, alpha = 0)
cv.lasso.pollweeks <- cv.glmnet(x = x.train, y = y.train, alpha = 1)
cv.enet.pollweeks <- cv.glmnet(x = x.train, y = y.train, alpha = 0.5)

# Get minimum lambda values 
lambda.min.ridge <- cv.ridge.pollweeks$lambda.min
lambda.min.lasso <- cv.lasso.pollweeks$lambda.min
lambda.min.enet <- cv.enet.pollweeks$lambda.min

# Predict on training data using lambda values that minimize MSE.
mse.ridge <- mean((predict(ridge.pollsweeks, s = lambda.min.ridge, newx = x.train) - y.train)^2)
mse.lasso <- mean((predict(lasso.pollsweeks, s = lambda.min.lasso, newx = x.train) - y.train)^2)
mse.enet <- mean((predict(enet.pollsweeks, s = lambda.min.enet, newx = x.train) - y.train)^2)

# Generate plot comparing coefficients for each of the weeks. 
d.coefplot <- data.frame("OLS" = coef(ols.pollweeks)[-1], 
                         "Ridge" = coef(ridge.pollsweeks, s = lambda.min.ridge)[-1], 
                         "Lasso" = coef(lasso.pollsweeks, s = lambda.min.lasso)[-1], 
                         "Elastic Net" = coef(enet.pollsweeks, s = lambda.min.enet)[-1]) %>% 
  rownames_to_column("coef_name") %>% 
  pivot_longer(cols = -coef_name, names_to = "method", values_to = "coef_est") %>% 
  mutate(week = rep(0:30, each = 4))

d.coefplot[which(is.na(d.coefplot$coef_est)),]$coef_est <- 0 

d.coefplot %>%
  ggplot(aes(x = coef_est, y = reorder(coef_name, -week), color = method)) +
  geom_segment(aes(xend = 0, yend = reorder(coef_name, -week)), alpha = 0.5, lty = "dashed") +
  geom_vline(aes(xintercept = 0), lty = "dashed") +   
  geom_point() + 
  labs(x = "Coefficient Estimate", 
       y = "Coefficient Name", 
       title = "Comparison of Coefficients Across Regularization Methods") + 
  theme_classic()

# First check how many weeks of polling we have for 2024. 
weeks_range <- d_pollav_natl %>% 
  filter(year == 2024) %>% 
  select(weeks_left) %>% 
  distinct() %>% 
  range()

x.train <- d_poll_weeks_train %>%
  ungroup() %>% 
  select(all_of(paste0("poll_weeks_left_", 4:30))) %>% 
  as.matrix()
y.train <- d_poll_weeks_train$pv2p
x.test <- d_poll_weeks_test %>%
  ungroup() %>% 
  select(all_of(paste0("poll_weeks_left_", 4:30))) %>% 
  as.matrix()

# Using elastic-net for simplicity. 
enet.poll <- cv.glmnet(x = x.train, y = y.train, alpha = 0.5)
lambda.min.enet.poll <- enet.poll$lambda.min

# Predict 2024 national pv2p share using elastic-net. 
polls.pred <- predict(enet.poll, s = lambda.min.enet.poll, newx = x.test)
rownames(polls.pred) <- c("Kamala Harris", "Donald Trump")
colnames(polls.pred) <- c("predicted_pv2p")

polls.pred <- as.data.frame(polls.pred)
polls.pred <- polls.pred %>% 
  mutate(predicted_pv2p_rescaled = 100*predicted_pv2p / sum(predicted_pv2p)) %>% 
  select(predicted_pv2p_rescaled)

# Render the table with row coloring, displaying only Rescaled Vote Share
knitr::kable(polls.pred, col.names = c("Candidate", "Predicted Two-Party Vote Share (%)")) %>%
  kableExtra::kable_styling(full_width = FALSE) %>%
  kableExtra::row_spec(1, background = "lightblue") %>%  # Color Kamala Harris row blue
  kableExtra::row_spec(2, background = "lightpink")
```

These predictions are derived from polling data up to 30 weeks before the election trained on historical data from the election in 1968 to the most recent one in 2020, using Elastic Net regression for its balance between feature selection and regularization. However, my prediction of Harris receiving `r round(polls.pred[1, 1], 1)`% of the popular vote and Trump `r round(polls.pred[2, 1], 1)`% has a margin of error, as reflected in the MSE of `r round(mse.enet, 2)`, which means the average difference between my prediction and true vote share in this case would be around `r round(sqrt(mse.enet), 1)` percentage points.

### How do state-level polls differ from national level polls?

While national level polls may not perfectly explain election results, **can state-level polls can provide more granular insights** into voter preferences and behavior? Each state is a unique battleground in U.S. elections due to the electoral college system. Therefore, state-level polls offer a closer approximation to how individual states may swing, allowing for a finer understanding of electoral outcomes.

To demonstrate this, I used state-level polling data to build a **predictive model for the 2024 presidential election** using regularized regression methods. The regression plots visualizes the coefficients estimated for each of the weeks leading up to the election across different regression methods: OLS, Ridge, Lasso, and Elastic Net, using polling data from 1972 to 2020.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
# OLS: Democratic candidates pv2p on November polling average. 
ols.nov.1.state <- lm(pv2p ~ nov_poll, data = subset(d_poll_nov_state, party == "DEM"))

# OLS: Party-stacked pv2p on November polling average.
ols.nov.2.state <- lm(pv2p ~ nov_poll, data = d_poll_nov_state)

# Create dataset of polling average by week until the election.
d_poll_weeks_state <- d_pollav_state %>% 
  group_by(year, state, party, weeks_left) %>%
  summarize(mean_poll_week = mean(poll_support)) %>% 
  filter(weeks_left <= 30) %>% 
  pivot_wider(names_from = weeks_left, values_from = mean_poll_week) %>% 
  left_join(d_pvstate_wide, by = c("year", "state"))

# Split into training and testing data based on inclusion or exclusion of 2024.
d_poll_weeks_train_state <- d_poll_weeks_state %>% 
  filter(year <= 2020)
d_poll_weeks_test_state <- d_poll_weeks_state %>% 
  filter(year == 2024)

colnames(d_poll_weeks_state)[4:34] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_train_state)[4:34] <- paste0("poll_weeks_left_", 0:30)
colnames(d_poll_weeks_test_state)[4:34] <- paste0("poll_weeks_left_", 0:30)

# Comparison of OLS and regularized regression methods.
ols.pollweeks.state <- lm(paste0("D_pv2p ~ ", paste0( "poll_weeks_left_", 0:30, collapse = " + ")), data = d_poll_weeks_train_state)

# Separate data into X and Y for training.
x.train.state <- d_poll_weeks_train_state %>%
  ungroup() %>% 
  select(all_of(starts_with("poll_weeks_left_"))) %>% 
  as.matrix()
y.train.state <- d_poll_weeks_train_state$D_pv2p

## Ridge
na_index_x_state <- apply(x.train.state, 1, function(row) any(is.na(row)))
na_index_y_state <- is.na(y.train.state)
na_combined_state <- na_index_x_state | na_index_y_state
x.train.state[is.na(x.train.state)] <- mean(x.train.state, na.rm = TRUE)
x.train.state <- x.train.state[!na_combined_state, ]
y.train.state <- y.train.state[!na_combined_state]
ridge.pollsweeks.state <- glmnet(x = as.matrix(x.train.state), y = y.train.state, alpha = 0)

## Lasso.
lasso.pollsweeks.state <- glmnet(x = x.train.state, y = y.train.state, alpha = 1) # Set lasso using alpha = 1.

## Elastic net.
enet.pollsweeks.state <- glmnet(x = x.train.state, y = y.train.state, alpha = 0.5) # Set elastic net using alpha = 0.5.

# Use cross-validated versions to find the optimal values of lambda that minimize the MSE of your predictions. 
cv.ridge.pollweeks.state <- cv.glmnet(x = x.train.state, y = y.train.state, alpha = 0)
cv.lasso.pollweeks.state <- cv.glmnet(x = x.train.state, y = y.train.state, alpha = 1)
cv.enet.pollweeks.state <- cv.glmnet(x = x.train.state, y = y.train.state, alpha = 0.5)

# Get minimum lambda values
lambda.min.ridge.state <- cv.ridge.pollweeks.state$lambda.min
lambda.min.lasso.state <- cv.lasso.pollweeks.state$lambda.min
lambda.min.enet.state <- cv.enet.pollweeks.state$lambda.min

# Generate plot comparing coefficients for each of the weeks.
d.coefplot.state <- data.frame("OLS" = coef(ols.pollweeks.state)[-1], 
                         "Ridge" = coef(ridge.pollsweeks.state, s = lambda.min.ridge.state)[-1], 
                         "Lasso" = coef(lasso.pollsweeks.state, s = lambda.min.lasso.state)[-1], 
                         "Elastic Net" = coef(enet.pollsweeks.state, s = lambda.min.enet.state)[-1]) %>% 
  rownames_to_column("coef_name") %>% 
  pivot_longer(cols = -coef_name, names_to = "method", values_to = "coef_est") %>% 
  mutate(week = rep(0:30, each = 4))

d.coefplot.state[which(is.na(d.coefplot.state$coef_est)),]$coef_est <- 0 

d.coefplot.state %>%
  ggplot(aes(x = coef_est, y = reorder(coef_name, -week), color = method)) +
  geom_segment(aes(xend = 0, yend = reorder(coef_name, -week)), alpha = 0.4, lty = "dashed") +
  geom_vline(aes(xintercept = 0), lty = "dashed") +   
  geom_point() + 
  labs(x = "Coefficient Estimate", 
       y = "Coefficient Name", 
       title = "Comparison of Coefficients Across Regularization Methods") + 
  theme_classic()
```

I decided to use Elastic Net regression for prediction, as it yielded the most consistent coefficient estimates, accounting for the complexity in the data without overfitting.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
# Convert poll_weeks_left columns to numeric
d_poll_weeks_test_state <- d_poll_weeks_test_state %>%
  mutate(across(starts_with("poll_weeks_left_"), ~ as.numeric(.)))

# Elastic Net regression on historical data (1972-2020)
cv.enet.state <- cv.glmnet(x = as.matrix(x.train.state), y = y.train.state, alpha = 0)

# Extract the optimal lambda value from cross-validation
lambda.min.enet.state <- cv.enet.state$lambda.min

# Train Elastic Net model using optimal lambda
x.train.state <- x.train.state[,-c(1:7)]
enet.final.state <- glmnet(x = as.matrix(x.train.state), y = y.train.state, alpha = 0, lambda = lambda.min.enet)

# Predict on training data to assess model fit
y.train_pred_state <- predict(enet.final.state, newx = as.matrix(x.train.state))

# Calculate MSE for training data to check model performance
mse.enet.train.state <- mean((y.train.state - y.train_pred_state)^2)

# Prepare the test data for 2024
x.test.state <- d_poll_weeks_test_state %>%
  ungroup() %>% 
  select(all_of(starts_with("poll_weeks_left_"))) %>% 
  as.matrix()

x.test.state[is.na(x.test.state)] <- matrix(apply(x.test.state, 2, function(col) mean(col, na.rm = TRUE)), 
                                nrow = nrow(x.test.state), 
                                ncol = ncol(x.test.state), 
                                byrow = TRUE)
x.test.state <- apply(x.test.state, 2, function(col) {
  ifelse(is.na(col), mean(col, na.rm = TRUE), col)
})
x.test.state <- x.test.state[,-c(1:7)]
y.test_pred_state <- predict(enet.final.state, newx = x.test.state)

# Add predictions to the test dataset
d_poll_weeks_test_state$predicted_pv2p <- y.test_pred_state

# Select relevant columns
poll_table <- d_poll_weeks_test_state %>%
  select(party, year, state, predicted_pv2p) %>% 
  group_by(state) %>% 
  mutate(predicted_pv2p_rescaled = 100*predicted_pv2p[,"s0"] / sum(predicted_pv2p[,"s0"])) %>% 
  select(party, year, state, predicted_pv2p_rescaled)

# Use knitr::kable to render the table and color rows based on the party
knitr::kable(poll_table, col.names = c("Party", "Year", "State", "Predicted Two-Party Vote Share (%)")) %>%
  kableExtra::kable_styling(full_width = FALSE) %>%
  kableExtra::row_spec(which(poll_table$party == "DEM"), background = "lightblue") %>%  # Color DEM rows blue
  kableExtra::row_spec(which(poll_table$party == "REP"), background = "lightpink")     # Color REP rows red
```

An MSE of `r round(mse.enet.train.state, 2)` indicates that, on average, the model’s predictions deviate from the actual vote share by about `r round(sqrt(mse.enet.train.state), 1)` percentage points. This error in predicting vote share could mean missing whether a state swings to a particular party, especially in battleground states where races are often decided by margins of 1-2%.

### Insights from Silver and Morris

Nate Silver was a key figure behind FiveThirtyEight, one of the largest election forecasting platforms in recent election cycles, which is now led by Elliot Morris. Their contrasting perspectives on the roles of fundamentals (economic forces and demographic information) versus poll-based forecasts provide valuable insights into election prediction.

[Nate Silver (2024)](https://www.natesilver.net/p/model-methodology-2024) takes a more **case-by-case approach** to election prediction. He emphasizes the importance of accounting for specific events and conditions that may impact an election. For instance, he considered the unique influence of COVID-19 during the 2020 presidential elections and has since decided to remove those considerations as we transition from a pandemic to an endemic state. He also highlights the potential effects of independent candidates, like Robert F. Kennedy Jr., who recently suspended his presidential campaign. Additionally, Silver points out that with Biden (who has dropped out and been replaced by Harris) and Trump potentially facing off again, examining historical data from past presidential rematches becomes essential. This granularity in analysis ensures that all factors specific to the election year, particularly those affecting the candidates and significant societal changes, are adequately considered.

[Elliot Morris (2024)](https://abcnews.go.com/538/538s-2024-presidential-election-forecast-works/story?id=110867585), on the other hand, adopts a **big-picture perspective** in his forecasting model. He integrates polling averages—assigning different weights and scores to various pollsters—with fundamental data that captures the economic landscape, such as employment rates, consumer spending, inflation, and political metrics like presidential approval ratings. This combination allows Morris to provide a more holistic view of the electoral landscape.

Silver's focus on depth and Moris's comprehensive coverage of breadth brings me to a novel idea: **what if we approached election predictions from an individual or micro-level perspective?** Imagine a perfect world where we could obtain any information from voters, except for who they will actually vote for on election day. In such a scenario, we could segment voters based on their behaviors and motivations:
* Partisan Voters: These individuals would support their party regardless of candidate or any “game-changing” moments.
* Candidate-Centric Voters: A smaller group, who decide based on the individual candidate rather than party affiliation.
* Challenger Voters: Voters who lean towards challengers when dissatisfied with current economic and social conditions, driven by a desire for change.
* Swing Voters: Those who fluctuate between parties based on specific issues or candidate qualities.
* Situational Voters: Their likelihood to show up could depend on various circumstances, such as pressing issues or the overall campaign dynamics.

If we could quantify these segments, we might be able to predict election outcomes accurately. **However, this perfect world with perfect information lives only in our imagination, much like the ideal of a perfect democracy**. In reality, voters' motivations are complex, often influenced by emotions, misinformation, and social dynamics that are difficult to quantify. While we may never achieve that perfect clarity, this attempt to understand voters' decision-making process can offer insights to how democracy works in practice.

*Code developed with the assistance of ChatGPT.*
